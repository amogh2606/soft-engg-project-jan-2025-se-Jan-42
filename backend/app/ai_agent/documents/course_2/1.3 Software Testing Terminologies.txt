[Music] hello everyone welcome back we are still doing week one i am going to use this module and the one that immediately follows this to tell you about certain standard glossary of terms and terminologies that are used in software testing as a standard practice across the world so we'll use two modules of lectures this one and the next one to understand all software testing terminologies for this course and those that are standard in nature so as an outline for this module you might have heard of terms like verification validation and testing i'll explain the difference between them i'll tell you what are all the testing terms that we will use what are the types of testing as a classification and what are the typical activities that we will pursue during testing so this is the definition of software testing that i borrowed from wikipedia it says software testing is the act of examining the artifacts which is your code your design architecture documents requirements documents those are the artifacts you examine those artifacts and the behavior of the software when these artifacts are executed and test them by validation and verification we will see what these latter two terms are what is the goal of software testing it provides an objective independent view of the software and checks whether it follows the appropriate business capabilities that it is meant to satisfy and also is used as a means to evaluate the risks that are involved in code corresponding to the software so let us move on like i told you in the last class lot of ieee standards apply for software engineering and software testing also there is an ieee standard glossary of software engineering terminology called std 610 series ieee std 610 as per that glossary here are the definition of the terms verification and validation validation is the process of evaluating software at the very end of the software development activity to ensure that it's compliant to whatever standards it's supposed to meet and compliant to the requirements that it was written for and it is meant and works correctly for its intended usage so validation is the process of evaluating software at the end of the software development stage to ensure compliance with intended usage for requirements specifically verification on the other hand is the process of determining whether the products produced throughout the phases of software development life cycle each phase has a set of work products as we saw in the last lecture with these products fulfill their requirements their needs a process of checking for that is verification a lot of the testing that we will do in this course will lean towards verification towards the end we will talk about validation techniques also there are several other related areas each one a subject meriting its own course ah as a standalone course that are related to software testing one area that's very complement to software testing but has similar goals to ensure correctness and to find bugs is that of formal methods of formal verification it has three broad techniques model checking theorem proving and program analysis there are several nptel and other courses in these that you could use to learn about them we will not be doing formal methods in this course another related area is to validate or verify design and architecture requirements using what we call modeling them by using modeling languages and running simulations on these models to check if they behave as we intend them to be this is different from testing and this course will not cover modeling and simulation also a area that is very important for testing for testing to succeed which goes along with testing is that of software quality assurance i will briefly tell you what this is about sometime in this course another one is accreditation which is preparing a software to meet certain standards of certification by a body that certifies the software which involves testing we learn the techniques but we will not focus any other accreditation related activities in this course agile software methodologies predominantly follow what we call test driven development paradigm even though this is not directly software testing it's one important agile development methodology that is related to software testing promotes software testing so we will see one lecture we will devote one lecture for test driven development in short t d d let us move on and understand some testing terminologies you must have heard you know first lecture i used the term error you must have heard a synonymous term called defect then you must have heard oh there is something faulty in my code or you must have heard saying this whole code is failing this whole application is failing so what are these terms are they related are they one and the same so let's see what the standard glossary has got to tell about them so these are the definitions as per the standard glossary fault is a static defect in software means that it is something that the developer has himself or herself missed it could be a missing function or it could be a wrongly coded piece of code in the software that's a fault in the software when a fault occurs or it becomes visible during testing it manifests itself as a failure so a failure is an incorrect behavior of the fault that is visible to the external user typically a tester or a developer so a failure is an externally visible incorrect behavior with reference to some requirements that you are testing it for or some other expected behavior for your software so a failure is an external manifestation of a fault a fault the third one is an error when a failure happens or when a fault is present the state of your program or your software artifact the internal state of your program becomes incorrect it enters what we call an incorrect state so when that happens then we say that there is an error in the software so these are the definitions as per the standard glossary few other terms bug defects all these are always there in the context of software testing so like last time i told you right testing is as old as coding is how old it is you will be surprised to know that people like edison the first programmer aida lovelace they all talked about these terms there is a quote by edison where he uses the term bug and fault as follows so he says it has just been so in all my inventions not related to software but more on inventions the first step is an intuition and comes with a burst then difficulties arise and this thing gives out and then that bugs which he says are little faults and difficulties show themselves and it needs months of intense watching study and labor to fix them right so these are related to inventions failures and inventions but then they can also fail and then you have to rectify it er lovelace who is believed to be the first programmer ever explicitly uses the term error remember she used to do punch cards in charles babbage's analytical engine which is believed to be the first computer so ada lovely says an analyzing process must equally have been performed in order to furnish the analytical engine the then computer the first computer with the necessary operative data and here in may also lie a possible source of error simply speaking in current days terms what she is saying is that the program that you execute in a computer can have an error granted that the actual mechanism is unnerving in its processes the cards may give it wrong orders so errors are really old and software testing's goal is to find and fix them so in this course even though we talked about this glossary of fault failure error bug and defect we will use all these terms synonymously we won't really distinguish one from the other and in my lectures you might hear one for the other just remember that they all mean the same in this course they all mean that there is something wrong with the software artifact that i am testing moving on we know testing is all about writing test cases write a test case execute it see what happens in the code so what is a test case how does it look like if you want to define what a test case is that is what is given here a test case has two parts two main parts one is test case inputs which are given as the inputs to the software artifact that is being tested you run the software artifact typically you code on your inputs and then that one produces some outputs so you take those outputs and you see if those outputs are right or not how do you see that you take those outputs you call them actual outputs and match it to what you have written out as expected outputs if they match then you conclude that this test case has passed if they don't match then you conclude that the test case has failed failed test case could indicate an error if your goal is to use failed test case to find an error so what is a test case to repeat a test case consisting of test inputs and expected outputs after running a test case on your code if you match your expected outputs to the actual outputs produced by the code if they match then you say that the test case is passed that is the outcome of execution of the test case otherwise you say that the test case is failed a failed test case typically indicates an error test case also has what we call test case id and if you remember in the earlier lecture i told you about traceability right traceability basically says that what is this test case meant to test this test case is meant to test such a such a requirement so test case will also have traceability details and id for retrieving it for later use and so on but from the purpose of learning algorithms for test case design you always have to remember just two things test case consists of inputs and expected outputs we will move on and understand various kinds of ways of classifying testing so the first way is types of testing as it goes down the faces of the software development life cycle what are the types of testing that are done the first one is what we call unit testing which i told you in the earlier module is done by the developer during coding itself so it's a testing done by programmer programmer writes unit test cases executes them on the code checks whether the functionality that is written by him or her works correctly and if it doesn't rewrites the functionality and ensures that the error is removed after unit testing comes integration testing where maybe two programmers wrote two different methods they are putting together and testing if the two methods one calling the other let's say work correctly along with the interface of calling that is what is called integration testing so this is software integration testing when two software components are put together at a later stage you could also do system level integration testing where you put the software with the hardware and the database connect them to the server and then test so after integration testing when the full system level implementation is ready when you put it together on the platform and test it that's called system level testing after system level testing is what we call acceptance testing which is typically done just before release or soon after release by the end customer to ensure that the delivered software product meets all the requirements that it was committed to be written for a related term is what we call beta testing many times you would hear the term this is a beta release of a particular software roughly it means that this is a release that ready but it might have certain kinds of errors if you happen to encounter one of them please report it back to us and we'll strive to fix it so beta testing is done on what is called a beta version of the software typically by end users soon after release so i will illustrate these types of testing through a small example i hope by now all of you are comfortable with reading a picture like this so this diagram says that there are three classes there is a main class called p which has two subclasses or child classes a and b and class a has two methods m one and m two class b also has two methods m three and m four let's say us there are two developers developer one is writing code for methods m1 and m2 in class a developer two is writing code for methods m3 and m4 in class b right so when the developer tests whether he or she has written the functionality for m1 or m2 or m3 or m4 whether that has been written correctly then that is what we call unit testing right a method is thought of as a unit of software and as a standalone entity the method is being developed and tested by the developer to ensure that it is working correctly then that is referred to as unit testing when the method interfaces so if you look at this diagram method m1 calls m4 it also calls m3 method m3 in turn calls m4 again method m2 also calls m4 so these are what we call interfaces or the call interfaces where one method calls the other so if you try to see method m1 put together with m4 do they together work correctly along with their interface then you do what is called integration testing similarly if you check with the method m1 calling method m3 that is the interface between m1 and m3 is working correctly then that is also called integration testing that's the next stage after unit testing where two components two different pieces of software components are put together and their interfaces are tested for correctness the next level above is you might want to test this whole class p right class b has this class a class b child classes they have their respective methods but class b is meant to satisfy some functionality as per your design document so when you are testing the whole class b which includes the methods of classes a and classes b checking for its functionality to check whether it is fine this is what we call system testing at a software level there's one more level of system testing that is not illustrated in this picture which talks about taking this whole piece of software connecting it to the database connecting it to the hardware connecting it to the network and testing it at a system level so that is also system testing but here i have focused only on the software side of the system testing so i hope you understand the types of testing after this so apart from these let me just go back by one slide apart from these which goes through the faces of sdlc towards the end you also test the software to see if it meets several quality requirements or not so those are additional types of testing what i have shown you here in this slide is a very incomplete list as we move along in the course we will devote a couple of lectures to exclusively understand what these kinds of testings are so in addition to unit testing integration testing system testing acceptance testing people also test the software for functionality that's called functional testing people test test the software to see if it meets its peak levels stress testing a classical example would be let us say class 10 results of a particular state board are coming out and you know for the first two hours every class 10 student his or her parent and relative would be trying to access the system to check what the marks are to check the results are so for a certain period of time the software is subject to extreme peak load with several people accessing the software simultaneously after a few hours after a few days there may not be that high number of accesses so a specific kind of testing done to meet to evaluate if the software meets the requirements that under peak conditions is what we call stress testing another kind of quality testing is performance testing which is done to ensure that the software responds within a certain speed see the minute we enter our credit card or debit card into an atm machine we want the welcome screen and we want the menu right if it takes let's say even 10 seconds for it to load we get restless so performance testing tests for speed for the response time of software to see if it meets all the requirements the next important kind of testing is usability testing to see if the software has a user interface that is usable by all if the software is aesthetic in terms of its availability to different kinds of users subset of usability testing is also called accessibility testing like for example we know that many of these many of the software these days especially the web applications have what we call captcha right which is basically meant to check if a human being is accessing it so usually they show a distorted image and you have to type the characters in the image or you have to recognize something about the image but imagine a visually impaired person using the software this part of the captcha will not be useful for them so adjacent to the captcha logo there is usually an audio button which plays out a word or spells out a word that the visually impaired person can hear and type it and that's the captcha that validates the visually impaired person so usability testing tests for all aspects of usability including this kind of accessibility regression testing i told you briefly in the last lecture post maintenance a lot of effort goes into maintaining and adding new features to the software instead of testing the software all over again typically people reuse test cases and this whole process of reusing test case is a large area in testing called regression testing so we will devote one lecture to regression testing later so this is an incomplete list there are so many other quality parameters that you can test for later in the course we will talk a little more about these kinds of testing another classification on testing is the method that as a tester you would use for testing you would have heard terms like black box testing white box testing there's also something called a gray box testing which is between black box and white box so what are these black box testing is a method of testing where you examine your software artifact let's say code without looking at its internal design or the actual code itself you know the inputs to the software you know the requirements of the software you have the code as an executable artifact you design test cases purely based on inputs and the requirements that this software is supposed to meet and execute it on the code and see if the requirements are met or not you do not look at the internals of the code sometimes you may not even know how the code is written what its design is so if you are testing by using the software artifact as a black box that is without looking at its internals testing purely based on its inputs and requirements then we call it as black box testing the other one is white box testing where you are as a tester aware of the structure of the code or the software artifact that you are testing you know the language you know its design you know the classes the methods the hierarchies all the details of the design and you design test cases by looking into the structure of the code and then use them for testing that's called white box testing we will learn several techniques for black box and for white box testing in this course if i now try to put the types of testing that we saw just a little while ago against the methods of testing that you see in this slide this table will give you an overview of what is happening so unit integration system and acceptance testing can be black box or white box not acceptance but unit integration and system texting can be both black box and white box when it comes to the other non-functional parameters quality parameters most of them are black box usability performance stress load reliability all of them are black box testing techniques this part will become clear as we move on in the course you will be able to understand with better clarity what these differences are now we saw the types of testing we saw briefly the methods of testing now its time to understand what are the typical activities that are pursued in testing broadly a tester throughout the life cycle of a software development this thing has four kinds of activities first is to design test cases that then is to actually automate make them ready to be put in tool the third one would be to execute these test cases and observe the result and the fourth one would be to make some inferences from the result and as we saw in the sdlc there are lots of adjacent activities like like we have project managers we have test managers we have people who maintain regression tests test case documentation that is needed for reuse later and so on so what is test case design intake this is the most critical job in testing why is it important because if i don't design effective test cases as a tester then i might spend hours or even days doing testing and not find a single error an effectively designed test case can catch defects or errors in the software reasonably fast it's wrong to say that i spent several days and your software is so good that i couldn't find a single error maybe it is true but many a times the reason that people would give for such a thing would be that oh your test case has not been effectively designed so it's important to know various ways or what are the right ways of designing test cases such that i can test a software for certain kind of feature for a certain kind of coverage or for finding certain kinds of errors this part cannot be automated needs human intervention needs basic knowledge in computer science concepts like discrete maths and algorithms and a tester will also need to know the domain of the software that he or she is working on a testing domain like a software for banking and finance sector is going to be different from testing a software that works for the automobile industry right so domain knowledge the basics of mathematics and how to design effective test case is what this critical phase of software testing activity is going to be all about a large part of this course is going to deal deal with teaching you algorithms for test case design once you design test cases you have to get rem ready to be executed that process is what we call automation testing automation involves converting the test cases into executable scripts typically you have you might like for example suppose you have a code which is 4-5 000 lines of code quite standard to write a code like that but inside that your goal is to test if a particular method is working correctly or not you have to first be able to reach that method and then you have to be able to test the correctness of that method using whatever way you want to and then if something is wrong you need to be able to observe it as a change in output so test automation evaluates how to prepare and reach the parts of the code or software that you want to test on and how to observe whether it has been erroneous or not and deals with concepts that we call as observability and controllability i will teach you these in the next week to follow what do we mean by this and throughout the course we will learn a few techniques that will help us in test automation along with test case design we will see a few open source test automation tools in this course after automation comes execution you need some entity a tool to be able to execute the test case observe the results and see what is happening that can be almost fully automated there are several excellent open source and proprietary tools that are available if you happen to be working for a company after graduating the company will most likely have proprietary or open source tools that they have standardized for use within themselves in this course we will teach you how to use a few open source test automation and execution tools that you will use as you move on finally okay you've designed the test cases you figured out how to package you've executed them you have the results with you how good is the result does this indicate whether the software is correct or not have i actually found the bug that i meant to find is this test case passed or failed how am i going to analyze it all this is done in test evaluation this is again going to be very difficult to fully automate you need manual help manual help is also needed for another important reason so let's say you have tested a piece of code as i told you a class that contains several different methods and let's say to put together there are more than four five thousand lines of code and you realize that something is wrong there is an error but then where is the error the code is pretty big which method which statement is erroneous this is the problem of isolating faults a fault has occurred how do i isolate it to a particular region in the code that needs experience human knowledge and human intervention cannot be automated so test evaluation is the last step in software testing so just to go back briefly test case activities can be four parts test case design automation execution and evaluation in this course you will learn several techniques for test case design and use test automation tools to be able to execute them and maybe manually learn how to evaluate and isolate faults in these test cases all the material that i took in this course are taken from this textbook there is an older version of the textbook freely available online if you would like to use them thank you i'll stop here for this module you